name: GameRadar AI Ingestion Engine

on:
  # Ejecuta autom√°ticamente cada 6 horas
  schedule:
    - cron: '0 */6 * * *'  # 00:00, 06:00, 12:00, 18:00 UTC
  
  # Permite ejecuci√≥n manual desde GitHub Actions UI
  workflow_dispatch:
    inputs:
      region:
        description: 'Region to scrape (KR, IN, VN, CN, etc.)'
        required: false
        default: 'KR'
      source:
        description: 'Data source (liquipedia, opgg)'
        required: false
        default: 'liquipedia'
      limit:
        description: 'Number of players to scrape'
        required: false
        default: '50'

jobs:
  scrape-and-ingest:
    name: Scrape E-Sports Data ‚Üí Bronze Layer
    runs-on: ubuntu-latest
    
    strategy:
      # Ejecutar para m√∫ltiples regiones en paralelo
      matrix:
        region: ['KR', 'IN', 'VN', 'CN']
      # Continuar con otras regiones si una falla
      fail-fast: false
    
    steps:
      - name: üì¶ Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: üìö Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright==1.41.2
          pip install supabase==2.3.4
          pip install pydantic==2.6.1
          pip install loguru
          pip install tenacity
          pip install aiohttp
          pip install python-dotenv
      
      - name: üé≠ Install Playwright Browsers
        run: |
          playwright install chromium
          playwright install-deps chromium
      
      - name: üöÄ Run Bronze Ingestion Scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          REGION: ${{ matrix.region }}
          SOURCE: ${{ github.event.inputs.source || 'liquipedia' }}
          LIMIT: ${{ github.event.inputs.limit || '50' }}
        run: |
          echo "üéØ Starting ingestion for region: $REGION"
          echo "üìä Source: $SOURCE | Limit: $LIMIT"
          python bronze_ingestion.py
        timeout-minutes: 15
      
      - name: üìä Upload Logs (if failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: ingestion-logs-${{ matrix.region }}
          path: bronze_ingestion.log
          retention-days: 7
      
      - name: üí¨ Notify Success
        if: success()
        run: |
          echo "‚úÖ Ingestion completed successfully for ${{ matrix.region }}"
          echo "üìà Check Supabase bronze_raw_data table for new records"
  
  # Job adicional: CNN Brasil Ninja Scraper (solo para India)
  scrape-cnn-brasil:
    name: Ninja Scraper - CNN Brasil
    runs-on: ubuntu-latest
    
    steps:
      - name: üì¶ Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: üìö Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright==1.41.2
          pip install supabase==2.3.4
          pip install pydantic==2.6.1
          pip install loguru
          pip install requests
      
      - name: üé≠ Install Playwright Browsers
        run: |
          playwright install chromium
          playwright install-deps chromium
      
      - name: ü•∑ Run CNN Brasil Ninja Scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          PROXY_URL: ${{ secrets.PROXY_URL }}
        run: |
          echo "ü•∑ Running CNN Brasil Ninja Scraper..."
          python cnn_brasil_scraper.py
        timeout-minutes: 10
        continue-on-error: true  # Silent failure
      
      - name: üìä Upload Logs (if exists)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cnn-brasil-logs
          path: |
            cnn_brasil_scraper.log
            ninja_scraper.log
          retention-days: 3
          if-no-files-found: ignore
  
  # Job de resumen: Consolidar resultados
  summary:
    name: üìä Ingestion Summary
    runs-on: ubuntu-latest
    needs: [scrape-and-ingest, scrape-cnn-brasil]
    if: always()
    
    steps:
      - name: üìù Generate Summary
        run: |
          echo "# GameRadar AI - Ingestion Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Time:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Regions Scraped:** KR, IN, VN, CN" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Status" >> $GITHUB_STEP_SUMMARY
          echo "- Bronze Ingestion: ${{ needs.scrape-and-ingest.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- CNN Brasil Ninja: ${{ needs.scrape-cnn-brasil.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Check [Supabase Dashboard](https://app.supabase.io) for new data." >> $GITHUB_STEP_SUMMARY
      
      - name: üîî Send Notification (Optional)
        if: failure()
        run: |
          echo "‚ö†Ô∏è Some ingestion jobs failed. Check logs above."
          # Aqu√≠ podr√≠as agregar notificaciones a Slack, Discord, etc.
